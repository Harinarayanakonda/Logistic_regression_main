import pytest
import pandas as pd
import numpy as np
import os
from unittest.mock import patch
from src.core.ml import ModelInferrer
from src.core.preprocessing import PreprocessingPipelineOrchestrator
from src.utils.helpers import save_artifact, generate_timestamped_filename
from src.config.paths import TRAINED_MODELS_DIR, PREPROCESSING_ARTIFACTS_DIR
from src.config.settings import AppSettings
from sklearn.linear_model import LogisticRegression

@pytest.fixture(scope="module")
def mock_trained_artifact(sample_dataframe, tmp_path_factory):
    """
    Creates a mock trained model artifact (preprocessing pipeline + dummy model)
    and saves it to a temporary path.
    """
    # Use a fresh set of paths for this fixture to avoid conflicts
    temp_models_dir = tmp_path_factory.mktemp("inference_models")
    temp_prep_artifacts_dir = tmp_path_factory.mktemp("inference_prep_artifacts")
    
    # Ensure AppSettings are set for the fixture's context
    original_target_column = AppSettings.TARGET_COLUMN
    original_ordinal_map = AppSettings.ORDINAL_FEATURES_MAP
    AppSettings.TARGET_COLUMN = 'target'
    AppSettings.ORDINAL_FEATURES_MAP = {'categorical_ordinal': ['Low', 'Medium', 'High']}

    # 1. Fit Preprocessing Pipeline
    prep_pipeline = PreprocessingPipelineOrchestrator(
        target_column=AppSettings.TARGET_COLUMN,
        ordinal_features_map=AppSettings.ORDINAL_FEATURES_MAP
    )
    prep_pipeline.fit(sample_dataframe)
    processed_df = prep_pipeline.transform(sample_dataframe)

    # 2. Train a dummy model
    X_train = processed_df.drop(columns=[AppSettings.TARGET_COLUMN])
    y_train = processed_df[AppSettings.TARGET_COLUMN]
    
    model = LogisticRegression(random_state=AppSettings.RANDOM_STATE, solver='liblinear')
    model.fit(X_train, y_train)

    # 3. Save the combined artifact
    artifact_filename = generate_timestamped_filename("model_artifacts")
    artifact_path = os.path.join(temp_models_dir, artifact_filename)

    artifact_to_save = {
        'preprocessing_pipeline': prep_pipeline,
        'model': model,
        'target_column': AppSettings.TARGET_COLUMN,
        'fitted_features': prep_pipeline.get_feature_names_after_preprocessing()
    }
    save_artifact(artifact_to_save, artifact_path)

    # Yield the path and then clean up
    yield artifact_path

    # Reset AppSettings after the fixture completes
    AppSettings.TARGET_COLUMN = original_target_column
    AppSettings.ORDINAL_FEATURES_MAP = original_ordinal_map
    
    # Clean up the temporary directories
    if os.path.exists(temp_models_dir):
        import shutil
        shutil.rmtree(temp_models_dir)
    if os.path.exists(temp_prep_artifacts_dir):
        import shutil
        shutil.rmtree(temp_prep_artifacts_dir)

def test_model_inferrer_load_artifacts(mock_trained_artifact):
    inferrer = ModelInferrer()
    inferrer.load_inference_artifacts(mock_trained_artifact)
    assert inferrer.preprocessing_pipeline is not None
    assert inferrer.model is not None
    assert isinstance(inferrer.model, LogisticRegression)
    assert inferrer.fitted_features is not None

def test_model_inferrer_predict(mock_trained_artifact, sample_dataframe):
    inferrer = ModelInferrer()
    inferrer.load_inference_artifacts(mock_trained_artifact)

    # Prepare new raw data for inference (can be a single row or multiple)
    new_data = pd.DataFrame({
        'numeric_col_1': [10, 20],
        'numeric_col_2': [1.1, 2.2],
        'categorical_binary': ['Yes', 'No'],
        'categorical_nominal': ['A', 'B'],
        'categorical_ordinal': ['Low', 'Medium'],
        'has_missing': [0, 0] # Should be generated by pipeline
    })
    # The 'target' column should not be in the inference input

    predictions = inferrer.predict(new_data)
    assert isinstance(predictions, pd.Series)
    assert len(predictions) == len(new_data)
    assert all(p in [0, 1] for p in predictions) # Assuming binary classification

def test_model_inferrer_predict_proba(mock_trained_artifact, sample_dataframe):
    inferrer = ModelInferrer()
    inferrer.load_inference_artifacts(mock_trained_artifact)

    new_data = pd.DataFrame({
        'numeric_col_1': [10, 20],
        'numeric_col_2': [1.1, 2.2],
        'categorical_binary': ['Yes', 'No'],
        'categorical_nominal': ['A', 'B'],
        'categorical_ordinal': ['Low', 'Medium'],
        'has_missing': [0, 0]
    })

    probabilities = inferrer.predict_proba(new_data)
    assert isinstance(probabilities, pd.DataFrame)
    assert probabilities.shape == (len(new_data), 2) # For binary classification
    assert 'probability_0' in probabilities.columns
    assert 'probability_1' in probabilities.columns
    assert np.allclose(probabilities.sum(axis=1), 1.0) # Probabilities sum to 1

def test_model_inferrer_not_loaded_error():
    inferrer = ModelInferrer()
    with pytest.raises(RuntimeError, match="not loaded"):
        inferrer.predict(pd.DataFrame({'col': [1]}))

def test_model_inferrer_missing_features_error(mock_trained_artifact):
    inferrer = ModelInferrer()
    inferrer.load_inference_artifacts(mock_trained_artifact)

    # Provide data with missing critical features for the model
    malformed_data = pd.DataFrame({
        'numeric_col_1': [10],
        'some_other_col': [5] # Missing many expected features
    })
    with pytest.raises(ValueError, match="Missing features in processed data for inference"):
        inferrer.predict(malformed_data)